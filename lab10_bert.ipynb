{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 10: Explore BERT\n",
    "\n",
    "In this lab, we will practice using pre-trained BERT models provided by the HuggingFace `transformers` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from typing import List\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1. Explore Pretrained BERT Model\n",
    "\n",
    "In this task, you will explore the pretrained BERT model using the Hugging Face Transformers library. \n",
    "\n",
    "First, you will load a pretrained BERT model and the correponding tokenizer. If you use the default model string `'bert-base-uncased'`, it will automatically download the model.\n",
    "\n",
    "In our case, to avoid any network issue, you can follow these steps to load the model locally:\n",
    "- Download the `bert-base-uncased.zip` file from the course website and unzip it to the folder `bert-base-uncased` in the same directory as this notebook. \n",
    "- When you load the model, you simply specify the folder path `bert-base-uncased/` (which contains all model files) to the `from_pretrained()` function. \n",
    "- *Note* that don't exclude the last `/` in the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased/') # Make sure you download the model files first\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by counting the number of parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tensors:  199\n"
     ]
    }
   ],
   "source": [
    "n_tensors = 0\n",
    "for param in bert_model.parameters():\n",
    "    n_tensors += 1\n",
    "\n",
    "print(\"Number of tensors: \", n_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  109482240\n"
     ]
    }
   ],
   "source": [
    "n_params = 0\n",
    "for param in bert_model.parameters():\n",
    "    n_params += param.numel()\n",
    "\n",
    "print(\"Number of parameters: \", n_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, if you are interested in how the parameters are organized, you can print the model's `_modules` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('embeddings', BertEmbeddings(\n",
      "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "  (position_embeddings): Embedding(512, 768)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")), ('encoder', BertEncoder(\n",
      "  (layer): ModuleList(\n",
      "    (0-11): 12 x BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (intermediate_act_fn): GELUActivation()\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")), ('pooler', BertPooler(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      "))])\n"
     ]
    }
   ],
   "source": [
    "print(bert_model._modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, you can access the parameters at any layer of the model, by specifying the layer name and index. \n",
    "\n",
    "For example, if you want to check the the query matrix $W^Q$ in the self-attention layer of the first transformer block, you can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(bert_model._modules['encoder']._modules['layer'][0]._modules['attention']._modules['self']._modules['query'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the $W^Q$ matrix is implemented as a `nn.Linear` module.\n",
    "\n",
    "Also, the same inquiry can be simplified by using the `get_submodule()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "torch.Size([768, 768])\n"
     ]
    }
   ],
   "source": [
    "W_q = bert_model.get_submodule('encoder.layer.0.attention.self.query')\n",
    "print(W_q)\n",
    "print(W_q.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2. Get Contextual Embeddings from BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on and use the BERT model to get contextual embeddings for given texts.\n",
    "\n",
    "First, we prepare some sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I have a new CPU!',\n",
      " 'I have a new Intel CPU!',\n",
      " 'I have a new GPU!',\n",
      " 'I have a new NVIDIA GPU!']\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "        'I have a new CPU!\\n'\n",
    "        'I have a new Intel CPU!\\n'\n",
    "        'I have a new GPU!\\n'\n",
    "        'I have a new NVIDIA GPU!'\n",
    "    )\n",
    "\n",
    "sentences = text.split('\\n')\n",
    "pprint(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the `tokenize()` function of the previously initialized BERT tokenizer on each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'have', 'a', 'new', 'cpu', '!'],\n",
      " ['i', 'have', 'a', 'new', 'intel', 'cpu', '!'],\n",
      " ['i', 'have', 'a', 'new', 'gp', '##u', '!'],\n",
      " ['i', 'have', 'a', 'new', 'n', '##vid', '##ia', 'gp', '##u', '!']]\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "tokens_in_string: List[str] = []\n",
    "for sentence in sentences:\n",
    "    # Tokenize the sentence\n",
    "    tokens = bert_tokenizer.tokenize(sentence)\n",
    "    # Convert to lowercase and add to the list\n",
    "    tokens_in_string.append([token.lower() for token in tokens])\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "pprint(tokens_in_string)\n",
    "# You should expect to see the following output:\n",
    "# [['i', 'have', 'a', 'new', 'cpu', '!'],\n",
    "#  ['i', 'have', 'a', 'new', 'intel', 'cpu', '!'],\n",
    "#  ['i', 'have', 'a', 'new', 'gp', '##u', '!'],\n",
    "#  ['i', 'have', 'a', 'new', 'n', '##vid', '##ia', 'gp', '##u', '!']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that \"CPU\" and \"Intel\" are recognized as whole words, but \"NVIDIA\" and \"GPU\" are not. Thus, they appear as subwords such as \"##u\" \"##vid\" in the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results are not integer token IDs yet, so now use the `batch_encode()` function, with argument `return_tensors='pt'`, to convert each sentence to integer token IDs. Here `'pt'` is for PyTorch tensors.\n",
    "\n",
    "**Note**:\n",
    "- Each token is represented as an integer in `torch.int64` data type.\n",
    "- By default, the tokenizer adds special tokens `[CLS]` and `[SEP]` to the beginning and end of each sentence, which correpond to the token ID `101` and `102`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "[tensor([[  101,  1045,  2031,  1037,  2047, 17368,   999,   102]]),\n",
      " tensor([[  101,  1045,  2031,  1037,  2047, 13420, 17368,   999,   102]]),\n",
      " tensor([[  101,  1045,  2031,  1037,  2047, 14246,  2226,   999,   102]]),\n",
      " tensor([[  101,  1045,  2031,  1037,  2047,  1050, 17258,  2401, 14246,  2226,\n",
      "           999,   102]])]\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "token_ids_list: List[torch.Tensor] = []\n",
    "for sentence in sentences:\n",
    "    encoded = bert_tokenizer.batch_encode_plus(\n",
    "        [sentence], \n",
    "        return_tensors='pt'  \n",
    "    )\n",
    "    token_ids_list.append(encoded['input_ids']) \n",
    "\n",
    "### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test\n",
    "print(token_ids_list[0].dtype)\n",
    "pprint(token_ids_list)\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# torch.int64\n",
    "# [tensor([[  101,  1045,  2031,  1037,  2047, 17368,   999,   102]]),\n",
    "#  tensor([[  101,  1045,  2031,  1037,  2047, 13420, 17368,   999,   102]]),\n",
    "#  tensor([[  101,  1045,  2031,  1037,  2047, 14246,  2226,   999,   102]]),\n",
    "#  tensor([[  101,  1045,  2031,  1037,  2047,  1050, 17258,  2401, 14246,  2226,\n",
    "#            999,   102]])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now `\"CPU\"` is tokenized to `17368`, `\"Intel\"` to `13420`, while `\"GPU\"` to `[14246, 2226]`, and `\"NVIDIA\"` to `[1050, 17258,  2401]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `ids_to_tokens` dictionary to map integer token IDs back to token strings, and use `decode()` function to convert a list of token IDs back to a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "[SEP]\n",
      "cpu\n",
      "[CLS] i have a new cpu! [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.ids_to_tokens[101])\n",
    "print(bert_tokenizer.ids_to_tokens[102])\n",
    "print(bert_tokenizer.ids_to_tokens[17368])\n",
    "print(bert_tokenizer.decode(token_ids_list[0].squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in last example above, we `squeeze` the token IDs first, becaseu the encoded IDs are of dimension $1\\times N$, where $N$ is sentence length, because PyTorch uses first dimension as batch size.\n",
    "\n",
    "It indicates that we can tokenize multiple sentences in one batch by using the `batch_encode_plus()` function, and specify the argument `padding=True` to pad all sentences to the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  2031,  1037,  2047, 17368,   999,   102,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  1045,  2031,  1037,  2047, 13420, 17368,   999,   102,     0,\n",
      "             0,     0],\n",
      "        [  101,  1045,  2031,  1037,  2047, 14246,  2226,   999,   102,     0,\n",
      "             0,     0],\n",
      "        [  101,  1045,  2031,  1037,  2047,  1050, 17258,  2401, 14246,  2226,\n",
      "           999,   102]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_sentences = bert_tokenizer.batch_encode_plus(sentences, return_tensors='pt', padding=True, return_attention_mask=False, return_token_type_ids=False)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the returned dictionary contains an item keyed by `'input_ids'`, which is exactly the token IDs we need. \n",
    "\n",
    "**Note**:\n",
    "- It is a tensor of shape $B\\times N$, where $B$ is the batch size (here, $B=4$) and $N$ is the maximum sentence length in the batch.\n",
    "- The default padding token is `0`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we deliberately set `return_attention_mask=False` to show simpler results. \n",
    "\n",
    "If you set it to `True`, then then returned dictionary will also contain an item keyed by `'attention_mask'`, which is a tensor of shape $B\\times N$ with `1` for real tokens and `0` for padding tokens. This information is useful for follow-up computations.\n",
    "\n",
    "Try if you can get the attention mask tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "encoded_sentences = None\n",
    "attn_mask = None\n",
    "encoded_sentences = bert_tokenizer.batch_encode_plus(\n",
    "    sentences,\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "attn_mask = encoded_sentences['attention_mask']\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print(attn_mask)\n",
    "# You should expect to see the following output:\n",
    "# tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "#         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
    "#         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
    "#         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's obtain the contextual embeddings for the four target words `\"CPU\"`, `\"Intel\"`, `\"NVIDIA\"`, and `\"GPU\"` in our sentences.\n",
    "\n",
    "First, pass the token IDs in one batch to the BERT model to get the output object, which has a `last_hidden_state` attribute that contains the contextual embeddings.\n",
    "\n",
    "**Note**:\n",
    "- You can manually specify `input_ids` and `attention_mask` as the input arguments to the model.\n",
    "- Or you can directly pass the dictionary returned by `batch_encode_plus()` to the model, and use the `**` operator as most tutorials did:\n",
    "  - `outputs = model(**encoded_sentences)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "bert_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    ### START YOUR CODE ###\n",
    "    outputs = None\n",
    "    outputs = bert_model(\n",
    "        input_ids=encoded_sentences['input_ids'],\n",
    "        attention_mask=attn_mask\n",
    "    )\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test\n",
    "print(outputs.last_hidden_state.shape)\n",
    "# You should expect to see the following output:\n",
    "# torch.Size([4, 12, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for ``\"CPU\"`` and ``\"Intel\"``, you can directly use the output vectors at the corresponding positions, because they are recognized as whole words.\n",
    "\n",
    "Compute the average vector of `\"CPU\"`s in the first two sentences, and compute its cosine similarity with the vector of `\"Intel\"`.\n",
    "\n",
    "*Hint*:\n",
    "- Use `F.cosine_similarity()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_cpu_intel: 0.7551645040512085\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "vec_cpu1 = outputs.last_hidden_state[0, 5, :] # bert tokenizer adds [CLS] token at the beginning\n",
    "vec_cpu2 = outputs.last_hidden_state[1, 6, :]\n",
    "vec_cpu_avg = (vec_cpu1 + vec_cpu2) / 2\n",
    "\n",
    "vec_intel = outputs.last_hidden_state[1, 5, :]\n",
    "cos_cpu_intel = F.cosine_similarity(vec_cpu_avg, vec_intel, dim=-1)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test\n",
    "print('cos_cpu_intel:', cos_cpu_intel.item())\n",
    "# You should expect to see the following output:\n",
    "# cos_cpu_intel: 0.7551645636558533"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `\"NVIDIA\"` and `\"GPU\"`, it's a bit trickier, as you need to use the sum of subword vectors to get the vector of the whole word.\n",
    "\n",
    "In sentence 3, `\"GPU\"` is tokenized to `[14246, 2226]`, so you need to sum the vectors at these two positions.\n",
    "\n",
    "In sentence 4, `\"NVIDIA\"` is tokenized to `[1050, 17258,  2401]`, so you need to sum the vectors at these three positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_gpu_nv: 0.7273836135864258\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "vec_gpu1 = outputs.last_hidden_state[2, 5:7, :].sum(dim=0)\n",
    "vec_gpu2 = outputs.last_hidden_state[3, 8:10, :].sum(dim=0)\n",
    "vec_gpu = vec_gpu1 + vec_gpu2\n",
    "vec_nvidia = outputs.last_hidden_state[3, 5:8, :].sum(dim=0)\n",
    "cos_gpu_nv = F.cosine_similarity(vec_gpu, vec_nvidia, dim=0)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print('cos_gpu_nv:', cos_gpu_nv.item())\n",
    "# You should expect to see the following output:\n",
    "# cos_gpu_nv: 0.7273837327957153"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if `\"NVIDIA\"` is closer to `\"GPU\"` than `\"CPU\"`, and vice versa for `\"Intel\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_cpu_nv: 0.5931223630905151\n",
      "cos_gpu_intel: 0.5778647661209106\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "cos_cpu_nv = F.cosine_similarity(vec_cpu_avg, vec_nvidia, dim=-1)\n",
    "cos_gpu_intel = F.cosine_similarity(vec_gpu, vec_intel, dim=-1)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print('cos_cpu_nv:', cos_cpu_nv.item())\n",
    "print('cos_gpu_intel:', cos_gpu_intel.item())\n",
    "# You should expect to see the following output:\n",
    "# cos_cpu_nv: 0.5931224226951599\n",
    "# cos_gpu_intel: 0.5778647661209106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's interesting, right?\n",
    "\n",
    "How about the distance between the two products `\"CPU\"` and `\"GPU\"`? or between the two companies `\"Intel\"` and `\"NVIDIA\"`? Check it out yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_cpu_gpu: 0.6914963722229004\n",
      "cos_intel_nv: 0.6179741621017456\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "cos_cpu_gpu = F.cosine_similarity(vec_cpu_avg, vec_gpu, dim=-1)\n",
    "cos_intel_nv = F.cosine_similarity(vec_intel, vec_nvidia, dim=-1)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print('cos_cpu_gpu:', cos_cpu_gpu.item())\n",
    "print('cos_intel_nv:', cos_intel_nv.item())\n",
    "# You should expect to see the following output:\n",
    "# cos_cpu_gpu: 0.6914964914321899\n",
    "# cos_intel_nv: 0.6179742813110352"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T3. Access all hidden states\n",
    "\n",
    "Let's be more adventurous and access all hidden states returned by the BERT model.\n",
    "\n",
    "*Hint*: Simply set the argument `output_hidden_states=True` when calling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "13\n",
      "torch.Size([4, 12, 768])\n",
      "torch.Size([4, 12, 768])\n",
      "torch.Size([4, 12, 768])\n",
      "torch.Size([4, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "bert_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    ### START YOUR CODE ###\n",
    "    outputs = bert_model(**encoded_sentences, output_hidden_states=True)\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print(type(outputs.hidden_states))\n",
    "print(len(outputs.hidden_states))\n",
    "print(outputs.hidden_states[-1].shape)\n",
    "print(outputs.hidden_states[-2].shape)\n",
    "print(outputs.hidden_states[-3].shape)\n",
    "print(outputs.hidden_states[-4].shape)\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# <class 'tuple'>\n",
    "# 13\n",
    "# torch.Size([4, 12, 768])\n",
    "# torch.Size([4, 12, 768])\n",
    "# torch.Size([4, 12, 768])\n",
    "# torch.Size([4, 12, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the average vector of the word `\"CPU\"` in the first sentence, using the hidden states of the last **four** layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos: 0.9002150297164917\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "vec_last4 = outputs.hidden_states[-4:]\n",
    "cpu_last4 = [tensor[0,5,:] for tensor in vec_last4]\n",
    "vec_cpu_avg_last4 = torch.stack(cpu_last4).mean(dim=0)\n",
    "vec_cpu_avg_last4.shape\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "cos = F.cosine_similarity(vec_cpu_avg_last4, vec_cpu_avg, dim=0)\n",
    "print('cos:', cos.item())\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# cos: 0.9002149701118469"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T4. Fine tune a BERT model for text classification task\n",
    "\n",
    "For the last task, we will practice fine-tuning a BERT-based model for a text classification task -- sentiment analysis on IMDB movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 327327.10 examples/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 741661.60 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 779352.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two fields in this dataset:\n",
    "- `text`: a string, the review text\n",
    "- `label`: an integer, 0 for negative, 1 for positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0,\n",
      " 'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV '\n",
      "         'are usually underfunded, under-appreciated and misunderstood. I '\n",
      "         'tried to like this, I really did, but it is to good TV sci-fi as '\n",
      "         'Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap '\n",
      "         \"cardboard sets, stilted dialogues, CG that doesn't match the \"\n",
      "         'background, and painfully one-dimensional characters cannot be '\n",
      "         \"overcome with a 'sci-fi' setting. (I'm sure there are those of you \"\n",
      "         \"out there who think Babylon 5 is good sci-fi TV. It's not. It's \"\n",
      "         'clichéd and uninspiring.) While US viewers might like emotion and '\n",
      "         'character development, sci-fi is a genre that does not take itself '\n",
      "         'seriously (cf. Star Trek). It may treat important issues, yet not as '\n",
      "         \"a serious philosophy. It's really difficult to care about the \"\n",
      "         'characters here as they are not simply foolish, just missing a spark '\n",
      "         'of life. Their actions and reactions are wooden and predictable, '\n",
      "         \"often painful to watch. The makers of Earth KNOW it's rubbish as \"\n",
      "         'they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise '\n",
      "         \"people would not continue watching. Roddenberry's ashes must be \"\n",
      "         'turning in their orbit as this dull, cheap, poorly edited (watching '\n",
      "         'it without advert breaks really brings this home) trudging Trabant '\n",
      "         'of a show lumbers into space. Spoiler. So, kill off a main '\n",
      "         'character. And then bring him back as another actor. Jeeez! Dallas '\n",
      "         'all over again.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(imdb['test'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a DistilBERT model tokenizer to process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp/lib/python3.11/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a preprocessing function to tokenize the `text` field of an example with truncation, so that it does not exceed the maximum length of the model (512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_imdb(examples):\n",
    "    return tokenizer(examples['text'], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `.map()` function to apply the preocessing function to the entire dataset, and speed it up using `batched=True`\n",
    "\n",
    "(takes a few seconds to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:04<00:00, 5927.32 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:03<00:00, 6431.80 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:08<00:00, 6230.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_imdb = imdb.map(preprocess_imdb, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all `text` field are tokenized to `input_ids`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are '\n",
      " 'usually underfunded, under-appreciated and misunderstood. I tried to like '\n",
      " 'this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek '\n",
      " '(the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, '\n",
      " \"CG that doesn't match the background, and painfully one-dimensional \"\n",
      " \"characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are \"\n",
      " \"those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's \"\n",
      " 'clichéd and uninspiring.) While US viewers might like emotion and character '\n",
      " 'development, sci-fi is a genre that does not take itself seriously (cf. Star '\n",
      " \"Trek). It may treat important issues, yet not as a serious philosophy. It's \"\n",
      " 'really difficult to care about the characters here as they are not simply '\n",
      " 'foolish, just missing a spark of life. Their actions and reactions are '\n",
      " 'wooden and predictable, often painful to watch. The makers of Earth KNOW '\n",
      " 'it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" '\n",
      " \"otherwise people would not continue watching. Roddenberry's ashes must be \"\n",
      " 'turning in their orbit as this dull, cheap, poorly edited (watching it '\n",
      " 'without advert breaks really brings this home) trudging Trabant of a show '\n",
      " 'lumbers into space. Spoiler. So, kill off a main character. And then bring '\n",
      " 'him back as another actor. Jeeez! Dallas all over again.')\n",
      "[101,\n",
      " 1045,\n",
      " 2293,\n",
      " 16596,\n",
      " 1011,\n",
      " 10882,\n",
      " 1998,\n",
      " 2572,\n",
      " 5627,\n",
      " 2000,\n",
      " 2404,\n",
      " 2039,\n",
      " 2007,\n",
      " 1037,\n",
      " 2843,\n",
      " 1012,\n",
      " 16596,\n",
      " 1011,\n",
      " 10882,\n",
      " 5691,\n",
      " 1013,\n",
      " 2694,\n",
      " 2024,\n",
      " 2788,\n",
      " 2104,\n",
      " 11263,\n",
      " 25848,\n",
      " 1010,\n",
      " 2104,\n",
      " 1011,\n",
      " 12315,\n",
      " 1998,\n",
      " 28947,\n",
      " 1012,\n",
      " 1045,\n",
      " 2699,\n",
      " 2000,\n",
      " 2066,\n",
      " 2023,\n",
      " 1010,\n",
      " 1045,\n",
      " 2428,\n",
      " 2106,\n",
      " 1010,\n",
      " 2021,\n",
      " 2009,\n",
      " 2003,\n",
      " 2000,\n",
      " 2204,\n",
      " 2694,\n",
      " 16596,\n",
      " 1011,\n",
      " 10882,\n",
      " 2004,\n",
      " 17690,\n",
      " 1019,\n",
      " 2003,\n",
      " 2000,\n",
      " 2732,\n",
      " 10313,\n",
      " 1006,\n",
      " 1996,\n",
      " 2434,\n",
      " 1007,\n",
      " 1012,\n",
      " 10021,\n",
      " 4013,\n",
      " 3367,\n",
      " 20086,\n",
      " 2015,\n",
      " 1010,\n",
      " 10036,\n",
      " 19747,\n",
      " 4520,\n",
      " 1010,\n",
      " 25931,\n",
      " 3064,\n",
      " 22580,\n",
      " 1010,\n",
      " 1039,\n",
      " 2290,\n",
      " 2008,\n",
      " 2987,\n",
      " 1005,\n",
      " 1056,\n",
      " 2674,\n",
      " 1996,\n",
      " 4281,\n",
      " 1010,\n",
      " 1998,\n",
      " 16267,\n",
      " 2028,\n",
      " 1011,\n",
      " 8789,\n",
      " 3494,\n",
      " 3685,\n",
      " 2022,\n",
      " 9462,\n",
      " 2007,\n",
      " 1037,\n",
      " 1005,\n",
      " 16596,\n",
      " 1011,\n",
      " 10882,\n",
      " 1005,\n",
      " 4292,\n",
      " 1012,\n",
      " 1006,\n",
      " 1045,\n",
      " 1005,\n",
      " 1049,\n",
      " 2469,\n",
      " 2045,\n",
      " 2024,\n",
      " 2216,\n",
      " 1997,\n",
      " 2017,\n",
      " 2041,\n",
      " 2045,\n",
      " 2040,\n",
      " 2228,\n",
      " 17690,\n",
      " 1019,\n",
      " 2003,\n",
      " 2204,\n",
      " 16596,\n",
      " 1011,\n",
      " 10882,\n",
      " 2694,\n",
      " 1012,\n",
      " 2009,\n",
      " 1005,\n",
      " 1055,\n",
      " 2025,\n",
      " 1012,\n",
      " 2009,\n",
      " 1005,\n",
      " 1055,\n",
      " 18856,\n",
      " 17322,\n",
      " 2094,\n",
      " 1998,\n",
      " 4895,\n",
      " 7076,\n",
      " 8197,\n",
      " 4892,\n",
      " 1012,\n",
      " 1007,\n",
      " 2096,\n",
      " 2149,\n",
      " 7193,\n",
      " 2453,\n",
      " 2066,\n",
      " 7603,\n",
      " 1998,\n",
      " 2839,\n",
      " 2458,\n",
      " 1010,\n",
      " 16596,\n",
      " 1011,\n",
      " 10882,\n",
      " 2003,\n",
      " 1037,\n",
      " 6907,\n",
      " 2008,\n",
      " 2515,\n",
      " 2025,\n",
      " 2202,\n",
      " 2993,\n",
      " 5667,\n",
      " 1006,\n",
      " 12935,\n",
      " 1012,\n",
      " 2732,\n",
      " 10313,\n",
      " 1007,\n",
      " 1012,\n",
      " 2009,\n",
      " 2089,\n",
      " 7438,\n",
      " 2590,\n",
      " 3314,\n",
      " 1010,\n",
      " 2664,\n",
      " 2025,\n",
      " 2004,\n",
      " 1037,\n",
      " 3809,\n",
      " 4695,\n",
      " 1012,\n",
      " 2009,\n",
      " 1005,\n",
      " 1055,\n",
      " 2428,\n",
      " 3697,\n",
      " 2000,\n",
      " 2729,\n",
      " 2055,\n",
      " 1996,\n",
      " 3494,\n",
      " 2182,\n",
      " 2004,\n",
      " 2027,\n",
      " 2024,\n",
      " 2025,\n",
      " 3432,\n",
      " 13219,\n",
      " 1010,\n",
      " 2074,\n",
      " 4394,\n",
      " 1037,\n",
      " 12125,\n",
      " 1997,\n",
      " 2166,\n",
      " 1012,\n",
      " 2037,\n",
      " 4506,\n",
      " 1998,\n",
      " 9597,\n",
      " 2024,\n",
      " 4799,\n",
      " 1998,\n",
      " 21425,\n",
      " 1010,\n",
      " 2411,\n",
      " 9145,\n",
      " 2000,\n",
      " 3422,\n",
      " 1012,\n",
      " 1996,\n",
      " 11153,\n",
      " 1997,\n",
      " 3011,\n",
      " 2113,\n",
      " 2009,\n",
      " 1005,\n",
      " 1055,\n",
      " 29132,\n",
      " 2004,\n",
      " 2027,\n",
      " 2031,\n",
      " 2000,\n",
      " 2467,\n",
      " 2360,\n",
      " 1000,\n",
      " 4962,\n",
      " 8473,\n",
      " 4181,\n",
      " 9766,\n",
      " 1005,\n",
      " 1055,\n",
      " 3011,\n",
      " 1012,\n",
      " 1012,\n",
      " 1012,\n",
      " 1000,\n",
      " 4728,\n",
      " 2111,\n",
      " 2052,\n",
      " 2025,\n",
      " 3613,\n",
      " 3666,\n",
      " 1012,\n",
      " 8473,\n",
      " 4181,\n",
      " 9766,\n",
      " 1005,\n",
      " 1055,\n",
      " 11289,\n",
      " 2442,\n",
      " 2022,\n",
      " 3810,\n",
      " 1999,\n",
      " 2037,\n",
      " 8753,\n",
      " 2004,\n",
      " 2023,\n",
      " 10634,\n",
      " 1010,\n",
      " 10036,\n",
      " 1010,\n",
      " 9996,\n",
      " 5493,\n",
      " 1006,\n",
      " 3666,\n",
      " 2009,\n",
      " 2302,\n",
      " 4748,\n",
      " 16874,\n",
      " 7807,\n",
      " 2428,\n",
      " 7545,\n",
      " 2023,\n",
      " 2188,\n",
      " 1007,\n",
      " 19817,\n",
      " 6784,\n",
      " 4726,\n",
      " 19817,\n",
      " 19736,\n",
      " 3372,\n",
      " 1997,\n",
      " 1037,\n",
      " 2265,\n",
      " 13891,\n",
      " 2015,\n",
      " 2046,\n",
      " 2686,\n",
      " 1012,\n",
      " 27594,\n",
      " 2121,\n",
      " 1012,\n",
      " 2061,\n",
      " 1010,\n",
      " 3102,\n",
      " 2125,\n",
      " 1037,\n",
      " 2364,\n",
      " 2839,\n",
      " 1012,\n",
      " 1998,\n",
      " 2059,\n",
      " 3288,\n",
      " 2032,\n",
      " 2067,\n",
      " 2004,\n",
      " 2178,\n",
      " 3364,\n",
      " 1012,\n",
      " 15333,\n",
      " 4402,\n",
      " 2480,\n",
      " 999,\n",
      " 5759,\n",
      " 2035,\n",
      " 2058,\n",
      " 2153,\n",
      " 1012,\n",
      " 102]\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenized_imdb['test'][0]['text'])\n",
    "pprint(tokenized_imdb['test'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use `DataCollatorWithPadding` to pad the sequences in one batch to the longest sequence in the batch *dynamically*. \n",
    "\n",
    "This is a more efficient way than padding in the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define a DistilBERT model as an instance of `AutoModelForSequenceClassification` with 2 output classes (positive and negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp/lib/python3.11/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works on one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0592, -0.0413]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    input_tensor = torch.tensor(tokenized_imdb['test'][0]['input_ids']).unsqueeze(0)\n",
    "    attention_mask = torch.tensor(tokenized_imdb['test'][0]['attention_mask']).unsqueeze(0)\n",
    "    outputs = model(input_ids=input_tensor, attention_mask=attention_mask)\n",
    "    print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the `TrainingArguments` and `Trainer` from the `transformers` library to fine tune the model.\n",
    "\n",
    "- Training hyperparameters are set in `TrainingArguments`\n",
    "- `Trainer` takes model, tokenizer, dataset, data_collator, and training arguments as input\n",
    "- Call `trainer.train()` to start finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='output',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before launching the trainer, we will need an evaluation metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load('accuracy')\n",
    "# If you have problem connecting to huggingface, you can git clone the evaluate repo https://github.com/huggingface/evaluate.git\n",
    "# and copy the `metrics/accuracy` folder to your current directory\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!sh proxyon.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/r1v3r/output into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "os.environ['HUGGINGFACE_TOKEN'] = \"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\"\n",
    "os.environ['HTTP_PROXY'] = \"http://127.0.0.1:7890\"\n",
    "os.environ['HTTPS_PROXY'] = \"http://127.0.0.1:7890\"\n",
    "# 其余代码保持不变\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_imdb[\"train\"],\n",
    "    eval_dataset=tokenized_imdb[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are good to go!\n",
    "\n",
    "Note that it runs very slowly on CPU, and you better wrap up all the code to one Python script and run it on a GPU server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3126 [14:30<?, ?it/s]\n",
      "  0%|          | 0/3126 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp/lib/python3.11/site-packages/transformers/trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1642\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1644\u001b[0m )\n\u001b[0;32m-> 1645\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp/lib/python3.11/site-packages/transformers/trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1928\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1929\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1932\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1933\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1934\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1935\u001b[0m ):\n\u001b[1;32m   1936\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1937\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp/lib/python3.11/site-packages/transformers/trainer.py:2750\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2749\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2750\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2753\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp/lib/python3.11/site-packages/transformers/trainer.py:2775\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2774\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2775\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2776\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2777\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:763\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    761\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 763\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    772\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    773\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:581\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    579\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 581\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[1;32m    584\u001b[0m     x\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[1;32m    585\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    590\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:120\u001b[0m, in \u001b[0;36mEmbeddings.forward\u001b[0;34m(self, input_ids, input_embeds)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m    input_ids (torch.Tensor):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03membeddings)\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     input_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    122\u001b[0m seq_length \u001b[38;5;241m=\u001b[39m input_embeds\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Setting the position-ids to the registered buffer in constructor, it helps\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# when tracing the model without passing position-ids, solves\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# isues similar to issue #5664\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
